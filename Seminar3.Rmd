---
output: html_document
editor_options: 
  chunk_output_type: inline
---

# 1 Exploration

```{r}
test <- read.table(file="test.csv", sep=",", header=TRUE) 
train <- read.table(file="train.csv", sep=",", header=TRUE)
dataset = rbind(test, train)

```

**Target feature is biodegradability.**

**The target variable is encoded as ready biodegradable (1) and not ready biodegradable (2)**

```{r}
summary(dataset)
dataset
```

**How balanced is the target variable?**

Mean of target variable is 1.337 that is how we know there are more not ready biodegradable chemicals in our training dataset than ready biodegradable chemicals.

```{r}
table(dataset$Class)
table(test$Class)
table(train$Class)
```

```{r}
table_class <- (table(train$Class)) 
piepercent <- paste(round(100*table_class/sum(table_class), 2), "%") 
names(table_class) <- c("ready biodegradable", "not ready biodegradable") 
labels <- paste(names(table_class), piepercent) 
pie(table_class, labels = labels, main = "target variable", col=c("pink", "lightblue1")) 
#legend("topright", legend = piepercent, cex = 0.6, fill=c("pink", "lightblue1"))
```

**Are there any missing values present? If there are, choose a strategy that takes this into account.**

```{r}
is.na(dataset)

dataset[!complete.cases(dataset),] 
# 81/1055

#remove instances where one of values is missing 
train <- na.omit(train)
test <- na.omit(test)
dataset <- na.omit(dataset)
dataset
```

```{r}
table_class <- (table(dataset$Class)) 
piepercent <- paste(round(100*table_class/sum(table_class), 2), "%") 
names(table_class) <- c("ready biodegradable", "not ready biodegradable") 
labels <- paste(names(table_class), piepercent) 
pie(table_class, labels = labels, main = "target variable (without instances with missing values)", col=c("pink", "lightblue1")) 

```

**Most of your data is of the numeric type. Can you identify, by adopting exploratory analysis, whether some features are directly related to the target? What about feature pairs?**

```{r}

target_corr = abs(cor(dataset[,names(dataset)])[names(dataset)[42],])
target_corr <- sort(target_corr, decreasing = TRUE)
target_corr_top8 <- target_corr[2: 9]

#the most correlated features
#V1- 0.395
#V27-0.392
#V22-0.364
#V39-0.362
#V15-0.343
#V13-0.342
#V7- 0.332
#V33-0.315

plot(target_corr_top8, xlab="features", ylab = "correlation", main="features with biggest correalton to the target", xaxt = "n")
axis(1, at = c(1,2,3,4,5,6,7,8), labels = names(target_corr_top8))

```


```{r}
correlation_matrix <- cor(dataset)
correlation_matrix

library(corrplot)
corrplot(correlation_matrix, method = 'color')


```
```{r}
#subset
features = c("V1", "V5", "V7", "V10", "V11", "V13", "V15", "V17", "V18", "V22", "V24", "V27", "V33", "V36", "V38", "V39", "Class")
subset = dataset[features]
print(subset)
correlation_matrix_small <- cor(subset)
corrplot(correlation_matrix_small, method = 'color')
colors = c("#00AFBB", "#E7B800")
pairs(dataset[c("V1", "V27", "V39", "V12", "V13", "CLASS")], pch=19, col=colors[dataset$Class])
```

```{r}
library(MASS)
library(ROCR)
linearReg <- lm(V15 ~ V1, data = dataset)
plot(V15 ~ V1, data = dataset)
abline(linearReg)
linear <- lda(Class~., dataset)

```

**Produce at least three types of visualizations of the feature space and be prepared to argue why these visualizations were useful for your subsequent analysis.**

# 2 Modeling

***majority classifier***

```{r}
majority.class <- names(which.max(table(train$Class)))
majority.class
```

Accuracy of majority classifier:

```{r}
sum(test$Class == majority.class) / length(test$Class)
```

We want to achieve better accuracy than majority classifier.

```{r}
#function for classification accuracy
scores <- list()
CA <- function(observed, predicted)
{
  t <- table(observed, predicted)
  
  sum(diag(t)) / sum(t)
}
Sensitivity <- function(observed, predicted, pos.class)
{
  t <- table(observed, predicted)
  
  t[pos.class, pos.class] / sum(t[pos.class,])
}

# The specificity of a model
Specificity <- function(observed, predicted, pos.class)
{
  t <- table(observed, predicted)
  
  # identify the negative class name
  neg.class <- which(row.names(t) != pos.class)
  
  t[neg.class, neg.class] / sum(t[neg.class,])
}
scores.CA <- CA
scores.Sensitivity <- Sensitivity
scores.Specificity <- Specificity
```

***random classifier***

TODO

***decision tree***

```{r}
# . pomeni da pogledamo za vse atribute
library(rpart)
?rpart
decision_tree <- rpart(Class ~ ., data = test, method = "class")
plot(decision_tree)
text(decision_tree, pretty = 1)
```

```{r}
observed <- test$Class
predicted <- predict(decision_tree, test, type = "class")

confusion_matrix <- table(observed, predicted)
confusion_matrix

CA(observed, predicted)
```
```{r}
library(caret)
importance <- varImp(decision_tree, scale=FALSE)
importance_sorted <- sort(t(importance), decreasing = TRUE, index.return = TRUE)
print(t(importance))
importance_sorted
# summarize importance
print(importance)

#V15
#V1
#V17
#V10
#V12
#V13
#V11
#V16

```

Decision tree classifies better than majority classifier
***decision tree with subset***
```{r}

features
subset_of_test = test[features]

decision_tree_sub <- rpart(Class ~ ., data = subset_of_test, method = "class")
plot(decision_tree_sub)
text(decision_tree_sub, pretty = 1)

observed <- test$Class
predicted <- predict(decision_tree_sub, test, type = "class")

confusion_matrix <- table(observed, predicted)
confusion_matrix

CA(observed, predicted)
```
Accuracy with choosen subset is the same as it was before (with all features)

***KNN***

```{r}
library(CORElearn)

knn_n <- c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
results <- c()
for (x in nekaj) {
  cm.knn <- CoreModel(Class ~ ., data = train, model="knn", kInNN = x)
  
  predicted_knn <- predict(cm.knn, test, type="class")
  
  accuracy = CA(observed, predicted_knn)
  results[x] = accuracy
  print(accuracy)
}

results

#8  -> 8277512
#9  -> 8421053
#10 -> 8229665
#15 -> 8086124
#17 -> 8229665
#20 -> 8133971


```
LDA classifier
```{r}
train.lda <- lda(Class~., train)
train.lda.values <- predict(train.lda, train)
ldahist(train.lda.values$x[,1], g=train$Class)

train.lda.acc <- CA(test$Class, predict(train.lda, test)$class)

train.lda.conf <- table(test$Class, predict(train.lda, test)$class)

perf <- prediction(pred, test$Class)

```




