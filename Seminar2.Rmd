---
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r, warning=FALSE, message=FALSE}
library(corrplot)
library(dplyr)
library(rpart)
library(randomForest)
library(caret)
library(CORElearn)
library(PreProcess)
library(MASS)
library(rattle)
library(pROC)

```

# 1 Exploration

```{r, echo=T}
test <- read.table(file="test.csv", sep=",", header=TRUE) 
train <- read.table(file="train.csv", sep=",", header=TRUE)

set.seed(100)

```

**Target feature is biodegradability.**

**The target variable is encoded as ready biodegradable (1) and not ready biodegradable (2)**

```{r, echo=T}
summary(test)
test
```

**How balanced is the target variable?**

Mean of target variable is 1.337 that is how we know there are more not ready biodegradable chemicals in our training dataset than ready biodegradable chemicals.

```{r, echo=T}
table(train$Class)
```

```{r, echo=T}
table_class <- (table(train$Class)) 
piepercent <- paste(round(100*table_class/sum(table_class), 2), "%") 
names(table_class) <- c("ready biodegradable", "not ready biodegradable") 
labels <- paste(names(table_class), piepercent) 
pie(table_class, labels = labels, main = "target variable", col=c("pink", "lightblue1")) 
#legend("topright", legend = piepercent, cex = 0.6, fill=c("pink", "lightblue1"))
```

**Are there any missing values present? If there are, choose a strategy that takes this into account.**

```{r, echo=T}
is.na(train)

train[!complete.cases(train),] 
# 81/1055

#remove instances where one of values is missing 
train <- na.omit(train)
#test <- na.omit(test)
```

**First we normalize our data**

```{r, echo=T}
process <- preProcess(train, method=c("range"))
train <- predict(process, train)
test <- predict(process, test)
```

**Most of your data is of the numeric type. Can you identify, by adopting exploratory analysis, whether some features are directly related to the target? What about feature pairs?**

```{r, echo=T}

target_corr = abs(cor(train[,names(train)])[names(train)[42],])
target_corr <- sort(target_corr, decreasing = TRUE)
target_corr_top8 <- target_corr[2: 9]

#the most correlated features
#V1- 0.395
#V27-0.392
#V22-0.364
#V39-0.362
#V15-0.343
#V13-0.342
#V7- 0.332
#V33-0.315

plot(target_corr_top8, xlab="features", ylab = "correlation", main="features with biggest correalton to the target", xaxt = "n")
axis(1, at = c(1,2,3,4,5,6,7,8), labels = names(target_corr_top8))
```

```{r, echo=T}
correlation_matrix <- cor(train)

corrplot(correlation_matrix, method = 'color')

highly_correlated <- findCorrelation(correlation_matrix, names=TRUE, cutoff=0.9)

```

Scatter matrix

```{r, echo=T}
colors = c("#00AFBB", "#E7B800")
pairs(train[c("V1", "V27", "V39", "V12", "V13", "Class")], pch=19, col=c(2, 3))
```

Distribution

```{r, echo=T}
p1 <- hist(train[train$Class == 0,]$V1) 
p2 <- hist(train[train$Class == 1,]$V1)                     # centered at 4
plot( p1, col=rgb(0,0,1,1/4), xlab = "V1", ylab = "Count", main="Distribucija")  # first histogram
plot( p2, col=rgb(1,0,0,1/4), add=T)
```

**box plot for few features**

```{r, echo=T}
train1 = train[train$Class == 0,]
train2 = train[train$Class == 1,]
median1 <- apply(train1, 2, median)
median2 <- apply(train2, 2, median)
diff <- sort(abs(median1 - median2))
print(diff)
boxplot(V1 ~ Class, train)
```

**Produce at least three types of visualizations of the feature space and be prepared to argue why these visualizations were useful for your subsequent analysis.**

# 2 Modeling

Make target variable non numeric

```{r, echo=T}
train$Class <- as.character(train$Class)
train$Class <- as.factor(train$Class)
```

**Try to construct new features from existing ones.**

We looked at attribute information and saw 3 attributes that looked similar:

V5: F04[C-N]: Frequency of C-N at topological distance 4 V11: F03[C-N]: Frequency of C-N at topological distance 3 V34: F02[C-N]: Frequency of C-N at topological distance 2

We decided to add a new feature that combines these three features and we added the sum of V5, V11 and V34.

```{r, echo=T}

train <- train %>% mutate(V42 = V5 + V11 + V34)
test <- test %>% mutate(V42 = V5 + V11 + V34)
```

First lets split out test set in two sets we will actually use for determining accuracy

```{r, echo=T}

## Split the data so that we use 70% of it for training
train_index <- createDataPartition(y=train$Class, p=0.7, list=FALSE)

## Subset the data
train_set <- train[train_index, ]
test_set <- train[-train_index, ]
```

function for classification accuracy

```{r, echo=T}
scores <- list()
CA <- function(observed, predicted)
{
  t <- table(observed, predicted)
  
  sum(diag(t)) / sum(t)
}
Sensitivity <- function(observed, predicted, pos.class)
{
  t <- table(observed, predicted)
  
  t[pos.class, pos.class] / sum(t[pos.class,])
}

# The specificity of a model
Specificity <- function(observed, predicted, pos.class)
{
  t <- table(observed, predicted)
  
  # identify the negative class name
  neg.class <- which(row.names(t) != pos.class)
  
  t[neg.class, neg.class] / sum(t[neg.class,])
}
scores.CA <- CA
scores.Sensitivity <- Sensitivity
scores.Specificity <- Specificity
```

***majority classifier***

```{r, echo=T}

majority.class <- names(which.max(table(train$Class)))
majority.class

sum(train$Class == majority.class) / length(train$Class)
```

We want to achieve better accuracy than majority classifier.

***random classifier***

```{r, echo=T}
# Generate random predictions with equal probability for each class
predictions <- sample(c("0", "1"), size = nrow(train), replace = TRUE, prob = c(0.5, 0.5)) 
# Calculate the accuracy of the random classifier 
accuracy <- mean(predictions == train$Class)
accuracy
```

***decision tree***

```{r, echo=T}
train_control <- trainControl(method = "repeatedcv", number = 5, repeats=10)

# Train the model using the train function and the tuneGrid argument
train.dt <- train(Class ~ ., data = train, tuneLength = 50, 
          method = "rpart", metric = "Accuracy", trControl = train_control)
train.dt.roc <- roc(train$Class, predict(train.dt, train, type="prob")[,2])
train.dt
fancyRpartPlot(train.dt$finalModel)


```

```{r, echo=T}
observed <- test$Class
predicted <- predict(train.dt$finalModel, test, type = "class")

confusion_matrix <- table(observed, predicted)
confusion_matrix

CA(observed, predicted)
```

```{r, echo=T}
importance <- varImp(train.dt$finalModel, scale=FALSE)
importance_sorted <- arrange(importance, desc(Overall))
importance_sorted

```

***decision tree with subset of 15 the most important features***

```{r, echo=T}
top_features <- c(row.names(head(importance_sorted, 15)))
top_features <- append(top_features, "Class")
subset_of_train = train[top_features]

decision_tree_sub <- train(Class ~ ., data = subset_of_train, tuneLength = 50, 
                      method = "rpart", metric = "Accuracy", trControl = train_control)

observed <- test$Class
predicted <- predict(decision_tree_sub$finalModel, test, type = "class")

confusion_matrix <- table(observed, predicted)
confusion_matrix

CA(observed, predicted)
```

Accuracy with chosen subset is the same as it was before (with all features)

***Random forest***

```{r, echo=T}
set.seed(100)
tuneGrid <- expand.grid(.mtry = c(sqrt(ncol(train))))

trControl = trainControl(method='repeatedcv', number = 5, repeats = 10)
train.rf <- train(Class ~ ., data = train,method="rf",trControl=trControl, 
                  metric='Accuracy', tuneGrid=tuneGrid)
train.rf.roc <- roc(train$Class, predict(train.rf, train, type="prob")[,2])
train.rf
predicted <- predict(train.rf, test)
CA(observed, predicted)
```

```{r, echo=T}
library(dplyr)
var_imp <- varImp(train.rf)
## Create a plot of variable importance
var_imp %>%

        ## Create a ggplot object for aesthetic
        ggplot(aes(x=reorder(variables, importance), y=importance)) + 
        
        ## Plot the bar graph
        geom_bar(stat='identity') + 
        
        ## Flip the graph to make a horizontal bar plot
        coord_flip() + 
        
        ## Add x-axis label
        xlab('Variables') +
        
        ## Add a title
        labs(title='Random forest variable importance') + 
        
        ## Some layout for the plot
        theme_minimal()
```

***KNN***

```{r, echo=T}
knitr::opts_chunk$set(warning = FALSE) 

train$Class = factor(train$Class)

grid = expand.grid(k = c(1:20))

knn_model = train(Class ~., method= "knn", data = train, 
                    trControl = trainControl(method = "repeatedcv", number = 5, repeats = 10, search = "grid"),
                     tuneGrid = grid, preProcess = c("scale", "center"))

knn_model
best_k = which.max(knn_model$results$Accuracy)

qplot(1:20, knn_model$results$Accuracy, xlab = "k", ylab = "Performance(acc)", geom = c("point","line"))

train.knn <- knn_model

train.knn.roc <- roc(train$Class, predict(train.knn, train, type="prob")[,2])


```

On the graph we can see the best K for KNN in interval 1-20 is 5

***LDA classifier***

```{r, echo=T}
train.lda <- lda(Class~., train)
#train.lda.roc <- roc(train$Class, predict(train.lda, train, type="prob")[,2])
train.lda.values <- predict(train.lda, train)
ldahist(train.lda.values$x[,1], g=train$Class)

train.lda.acc <- CA(test$Class, predict(train.lda, test)$class)

train.lda.conf <- table(test$Class, predict(train.lda, test)$class)

train.lda.acc
train.lda.conf
```

***XGBoost***

```{r, echo = T}
trctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 10)
tune_grid <-  expand.grid(max_depth = c(3, 5, 7), 
                        nrounds = (1:10)*50,    # number of trees
                        # default values below
                        eta = 0.3,
                        gamma = 0,
                        subsample = 1,
                        min_child_weight = 1,
                        colsample_bytree = 0.6)

rf_fit <- train(Class ~ V39 + V30 + V1 + V15 + V27 + V13 + V38 + V10 + V34 + V2 + V37 +  V22 , data = train, method = "xgbTree",
                trControl=trctrl,
                tuneGrid = tune_grid,
                tuneLength = 10,
                verbosity = 0)
predicted <- predict(rf_fit, test)
CA(observed, predicted)
```
